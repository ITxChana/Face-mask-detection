{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNQo4F/wu/zQo8/M0KBfDP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"peOyuwaehclK"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","source":["import os, glob, random, matplotlib, cv2\n","import matplotlib.patches as mplpatches\n","from collections import Counter\n","import pandas as pd\n","from xml.etree import ElementTree as ET\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_addons as tfa\n","from tensorflow.keras.layers import Dense, Dropout, LayerNormalization\n","from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n","from tensorflow.keras import layers"],"metadata":{"id":"B7Anv02_hnWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install xmltodict"],"metadata":{"id":"O2kT3uKRhq_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import xmltodict"],"metadata":{"id":"oxNbIR2yhuTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotations_path = \"/kaggle/input/face-mask-detection/annotations\"\n","images_path = \"/kaggle/input/face-mask-detection/images\"\n","\n","CHANNELS = 3\n","IMAGE_SIZE = 224\n","INPUT_SIZE = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n","TRAIN_SET_PRCNT = 0.85"],"metadata":{"id":"I8L3WyYOhwF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_annotation(annotation_file):\n","\n","    objects = {\n","            \"xmin\":[],\n","            \"ymin\":[],\n","            \"xmax\":[],\n","            \"ymax\":[],\n","            \"name\":[],\n","            \"file\":[],\n","            \"width\":[],\n","            \"height\":[],\n","           }\n","\n","    tree = ET.parse(annotation_file)\n","\n","    for el in tree.iter():\n","        if 'size' in el.tag:\n","            for attr in list(el):\n","                if 'width' in attr.tag:\n","                    width = int(round(float(attr.text)))\n","                if 'height' in attr.tag:\n","                    height = int(round(float(attr.text)))\n","\n","        if 'object' in el.tag:\n","            for attr in list(el):\n","\n","                if 'name' in attr.tag:\n","                    name = attr.text\n","                    objects['name']+=[name]\n","                    objects['width']+=[width]\n","                    objects['height']+=[height]\n","                    objects['file']+=[annotation_file.split('/')[-1][0:-4]]\n","\n","                if 'bndbox' in attr.tag:\n","                    for dim in list(attr):\n","                        if 'xmin' in dim.tag:\n","                            xmin = int(round(float(dim.text)))\n","                            objects['xmin']+=[xmin]\n","                        if 'ymin' in dim.tag:\n","                            ymin = int(round(float(dim.text)))\n","                            objects['ymin']+=[ymin]\n","                        if 'xmax' in dim.tag:\n","                            xmax = int(round(float(dim.text)))\n","                            objects['xmax']+=[xmax]\n","                        if 'ymax' in dim.tag:\n","                            ymax = int(round(float(dim.text)))\n","                            objects['ymax']+=[ymax]\n","\n","    return objects\n"],"metadata":{"id":"K6t5x59hhwSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# list of paths to images and annotations\n","image_files = [\n","    f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))\n","]\n","annot_files = [\n","    f for f in os.listdir(annotations_path) if os.path.isfile(os.path.join(annotations_path, f))\n","]\n","\n","image_files.sort()\n","annot_files.sort()\n","\n","images, targets = [], []\n","\n","# loop over the annotations and images, preprocess them and store in lists\n","for i in range(0, len(annot_files)):\n","\n","    # Access bounding box coordinates\n","    annot = get_annotation(os.path.join(annotations_path, annot_files[i]))\n","\n","    top_left_x, top_left_y = annot['xmax'], annot['ymax']\n","    bottom_right_x, bottom_right_y = annot['xmin'], annot['ymin']\n","\n","    image = keras.utils.load_img(\n","        os.path.join(images_path, image_files[i]),\n","    )\n","    (w, h) = image.size[:2]\n","\n","    # resize train set images\n","    #if i < int(len(annot_files) * TRAIN_SET_PRCNT):\n","        # resize image if it is for training dataset\n","    #    image = image.resize((IMAGE_SIZE, IMAGE_SIZE))\n","    image = image.resize((IMAGE_SIZE, IMAGE_SIZE))\n","    # convert image to array and append to list\n","    #images.append(keras.utils.img_to_array(image))\n","    #targets.append((annot['xmax'], annot['ymax'],annot['xmin'], annot['ymin']))\n","\n","    # apply relative scaling to bounding boxes as per given image and append to list\n","    for i in range(0, len(top_left_x)):\n","        # convert image to array and append to list\n","        images.append(keras.utils.img_to_array(image))\n","        targets.append(\n","            (\n","                float(top_left_x[i]) / w,\n","                float(top_left_y[i]) / h,\n","                float(bottom_right_x[i]) / w,\n","                float(bottom_right_y[i]) / h,\n","            )\n","        )\n","\n","\n","\n","# Convert the list to numpy array, split to train and test dataset\n","(x_train), (y_train) = (\n","    np.asarray(images[: int(len(images) * TRAIN_SET_PRCNT)]),\n","    np.asarray(targets[: int(len(targets) * TRAIN_SET_PRCNT)]),\n",")\n","\n","(x_test), (y_test) = (\n","    np.asarray(images[int(len(images) * TRAIN_SET_PRCNT) :]),\n","    np.asarray(targets[int(len(targets) * TRAIN_SET_PRCNT) :]),\n",")"],"metadata":{"id":"UW3yuNnqhwUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_image = x_train[0]\n","fig, ax1 = plt.subplots(1, figsize=(15, 15))\n","im = input_image\n","# Display the image\n","ax1.imshow(im.astype(\"uint8\"))\n","\n","\n","input_image = cv2.resize(\n","    input_image, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA\n",")\n","#input_image = np.expand_dims(input_image, axis=0)\n","#preds = vit_object_detector.predict(input_image)[0]\n","#\n","(h, w) = (im).shape[0:2]\n","#\n","for i in range(0,3):\n","    top_left_x, top_left_y = int(y_train[i][0] * 224), int(y_train[i][1] * 224)\n","    #\n","    bottom_right_x, bottom_right_y = int(y_train[i][2] * 224), int(y_train[i][3] * 224)\n","    #\n","    box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n","    #\n","    ## Create the bounding box\n","    rect = mplpatches.Rectangle(\n","                            (top_left_x, top_left_y),\n","                            bottom_right_x - top_left_x,\n","                            bottom_right_y - top_left_y,\n","                            facecolor=\"none\",\n","                            edgecolor=\"red\",\n","                            linewidth=1,\n","                        )\n","\n","    ax1.add_patch(rect)\n","\n","plt.show()"],"metadata":{"id":"OyX6_mROhwWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count how many files there are in annotation path. It should be the same number as in images path\n","count = 0\n","# Iterate directory\n","for path in os.listdir(annotations_path):\n","    # check if current path is a file\n","    if os.path.isfile(os.path.join(annotations_path, path)):\n","        count += 1\n","print('File count:', count)"],"metadata":{"id":"clzFkmCChwYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's find out how many people are wearing mask, not wearing it or wearing it wrong:\n","img_names = []\n","xml_names = []\n","for dirname, _, filenames in os.walk(images_path):\n","    for filename in filenames:\n","        img_names.append(filename)\n","\n","img_annot_list = []\n","for i in img_names[:]:\n","    with open(os.path.join(annotations_path,i[:-4]+\".xml\")) as fd:\n","        doc = xmltodict.parse(fd.read())\n","    temp = doc[\"annotation\"][\"object\"]\n","    if type(temp) == list:\n","        for i in range(len(temp)):\n","            img_annot_list.append(temp[i][\"name\"])\n","    else:\n","        img_annot_list.append(temp[\"name\"])\n","\n","\n","classes = Counter(img_annot_list).keys()\n","target = Counter(img_annot_list).values()\n","print(list(zip(classes,target)))"],"metadata":{"id":"jDlm3aYPhwac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["matplotlib.pyplot.pie(target,\n","                      labels=classes,\n","                      colors=['green', 'yellow', 'red'],\n","                      pctdistance=0.8,\n","                      shadow=True,\n","                      labeldistance=1.1,\n","                      startangle=75,\n","                      radius=2,\n","                      counterclock=True)"],"metadata":{"id":"AhamHqdNhwfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_df_annotations():\n","    objects = {\n","            \"xmin\":[],\n","            \"ymin\":[],\n","            \"xmax\":[],\n","            \"ymax\":[],\n","            \"name\":[],\n","            \"file\":[],\n","            \"width\":[],\n","            \"height\":[],\n","           }\n","\n","    for an_path in glob.glob(annotations_path+\"/*.xml\"):\n","        tree = ET.parse(an_path)\n","\n","        for el in tree.iter():\n","            if 'size' in el.tag:\n","                for attr in list(el):\n","                    if 'width' in attr.tag:\n","                        width = int(round(float(attr.text)))\n","                    if 'height' in attr.tag:\n","                        height = int(round(float(attr.text)))\n","\n","            if 'object' in el.tag:\n","                for attr in list(el):\n","\n","                    if 'name' in attr.tag:\n","                        name = attr.text\n","                        objects['name']+=[name]\n","                        objects['width']+=[width]\n","                        objects['height']+=[height]\n","                        objects['file']+=[an_path.split('/')[-1][0:-4]]\n","\n","                    if 'bndbox' in attr.tag:\n","                        for dim in list(attr):\n","                            if 'xmin' in dim.tag:\n","                                xmin = int(round(float(dim.text)))\n","                                objects['xmin']+=[xmin]\n","                            if 'ymin' in dim.tag:\n","                                ymin = int(round(float(dim.text)))\n","                                objects['ymin']+=[ymin]\n","                            if 'xmax' in dim.tag:\n","                                xmax = int(round(float(dim.text)))\n","                                objects['xmax']+=[xmax]\n","                            if 'ymax' in dim.tag:\n","                                ymax = int(round(float(dim.text)))\n","                                objects['ymax']+=[ymax]\n","\n","    return objects\n","\n","objects = get_df_annotations()"],"metadata":{"id":"eqnhAZyziDUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(objects)\n","df.head(10)"],"metadata":{"id":"_m7XVA5giDcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_name_str = 'maksssksksss737'\n","filterr = df[df['file'] == file_name_str]\n","# path\n","path = fr'/kaggle/input/face-mask-detection/images/{file_name_str}.png'\n","\n","# Reading an image in default mode\n","image = cv2.imread(path)\n","# Window name in which image is displayed\n","window_name = 'Image'\n","\n","# Blue color in BGR\n","color = (255, 0, 0)\n","\n","# Line thickness of 2 px\n","thickness = 2\n","\n","# Using cv2.rectangle() method\n","# Draw a rectangle with blue line borders of thickness of 2 px\n","cv2.rectangle(image, (46, 71), (28, 55), color, thickness) #(xmax,ymax), (xmin,ymin)\n","cv2.rectangle(image, (111, 78), (98, 62), color, thickness)\n","cv2.rectangle(image, (193, 90), (159, 50), color, thickness)\n","cv2.rectangle(image, (313, 80), (293, 59), color, thickness)\n","cv2.rectangle(image, (372, 72), (352, 52), color, thickness)\n","cv2.rectangle(image, (241, 73), (228, 53), color, thickness)\n","\n","# Displaying the image\n","plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"],"metadata":{"id":"Ex67yRFSiLVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"X9LleE2riLa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = list(zip(df.width, df.height))\n","cntr = dict(Counter(result).most_common(10))\n","df_aux = pd.DataFrame.from_dict(Counter(cntr), orient='index', columns=['Width_height'])\n","df_aux.plot.bar()\n","plt.ylabel('Counts per width-height pair')\n","plt.xlabel('Width-height pair')\n","plt.title('Are the images the same size?')"],"metadata":{"id":"PNJN_IyBiLhQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","SEED = 42\n","DROPOUT = 0.1\n","NUM_HEADS = 8\n","ACTIVATION = \"relu\" # Activation function\n","PADDING = \"VALID\"\n","EPSILON = 1e-6\n","EPOCHS = 30\n","PATCH_SIZE = 32 # Size of the patches to be extracted from the input images\n","NUM_LAYERS = 8\n","WEIGHT_DECAY = 1e-4\n","BATCH_SIZE = 4\n","MLP_DIM = 128\n","PROJECTION_DIM = 64\n","LR = 1e-3"],"metadata":{"id":"Hm_IgWRpiUZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Implement multilayer-perceptron (MLP)\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x"],"metadata":{"id":"aV209jvIiUe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    #     Override function to avoid error while saving model\n","    def get_config(self):\n","        config = super().get_config().copy()\n","        config.update(\n","            {\n","                \"input_shape\": input_shape,\n","                \"patch_size\": patch_size,\n","                \"num_patches\": num_patches,\n","                \"projection_dim\": projection_dim,\n","                \"num_heads\": num_heads,\n","                \"transformer_units\": transformer_units,\n","                \"transformer_layers\": transformer_layers,\n","                \"mlp_head_units\": mlp_head_units,\n","            }\n","        )\n","        return config\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=PADDING,\n","        )\n","        # return patches\n","        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])\n"],"metadata":{"id":"SFMN10zxiUjz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Display patches for an input image\n","\n","random_num = random.randint(0, len(x_train)-1)\n","\n","plt.figure(figsize=(4, 4))\n","plt.imshow(x_train[random_num]/255.)\n","plt.axis(\"off\")\n","\n","patches = Patches(PATCH_SIZE)(tf.convert_to_tensor([x_train[random_num]]))\n","print(f\"Image size: {IMAGE_SIZE} X {IMAGE_SIZE}\")\n","print(f\"Patch size: {PATCH_SIZE} X {PATCH_SIZE}\")\n","print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n","\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(6, 6))\n","\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = tf.reshape(patch, (PATCH_SIZE, PATCH_SIZE, CHANNELS))\n","    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n"],"metadata":{"id":"y-esQA7Oia7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PatchEncoder(layers.Layer):\n","    # Implement the patch encoding layer\n","\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    # Override function to avoid error while saving model\n","    def get_config(self):\n","        config = super().get_config().copy()\n","        config.update(\n","            {\n","                \"input_shape\": input_shape,\n","                \"patch_size\": patch_size,\n","                \"num_patches\": num_patches,\n","                \"projection_dim\": projection_dim,\n","                \"num_heads\": num_heads,\n","                \"transformer_units\": transformer_units,\n","                \"transformer_layers\": transformer_layers,\n","                \"mlp_head_units\": mlp_head_units,\n","            }\n","        )\n","        return config\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded\n"],"metadata":{"id":"0E5ObabPibFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_vit_object_detector(\n","                                input_shape,\n","                                patch_size,\n","                                num_patches,\n","                                projection_dim,\n","                                num_heads,\n","                                transformer_units,\n","                                transformer_layers,\n","                                mlp_head_units):\n","\n","\n","    inputs = layers.Input(shape=input_shape)\n","    # Create patches\n","    patches = Patches(PATCH_SIZE)(inputs)\n","    # Encode patches\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=EPSILON)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=DROPOUT\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=EPSILON)(x2)\n","        # MLP\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=DROPOUT)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=EPSILON)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(DROPOUT*3)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=DROPOUT*3)\n","\n","    bounding_box = layers.Dense(4)(\n","                                    features\n","                                  )  # Final four neurons that output bounding box\n","\n","    # return Keras model.\n","    return keras.Model(inputs=inputs, outputs=bounding_box)\n"],"metadata":{"id":"kLaTA3rSiry6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.python.client import device_lib\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')), \"\\n\")\n","\n","print(device_lib.list_local_devices())\n","\n","tf.debugging.set_log_device_placement(True)\n","tf.random.set_seed(SEED)"],"metadata":{"id":"ulgGHsTGir4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n","\n","    optimizer = tfa.optimizers.AdamW(\n","                                        learning_rate=learning_rate,\n","                                        weight_decay=weight_decay\n","                                    )\n","\n","    # Compile model.\n","    model.compile(\n","                    optimizer=optimizer,\n","                    loss=[keras.losses.MeanSquaredError(), tfa.losses.GIoULoss()],\n","                    #metrics=[tf.keras.metrics.MeanIoU(num_classes=len(classes)+1)]\n","\n","                  )\n","\n","    checkpoint_filepath = \"logs/\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_loss\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","                            x=x_train,\n","                            y=y_train,\n","                            batch_size=batch_size,\n","                            epochs=num_epochs,\n","                            validation_split=0.25,\n","                            callbacks=[\n","                                        checkpoint_callback,\n","                                        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n","                                      ],\n","                         )\n","\n","    return history\n","\n","\n","num_patches = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","\n","\n","# Size of the transformer layers\n","transformer_units = [\n","                        PROJECTION_DIM * 2,\n","                        PROJECTION_DIM,\n","                    ]\n","\n","transformer_layers = 6\n","mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n","\n","\n","history = []\n","num_patches = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","\n","vit_object_detector = create_vit_object_detector(\n","                                                    INPUT_SIZE,\n","                                                    PATCH_SIZE,\n","                                                    num_patches,\n","                                                    PROJECTION_DIM,\n","                                                    NUM_HEADS,\n","                                                    transformer_units,\n","                                                    transformer_layers,\n","                                                    mlp_head_units,\n","                                                )"],"metadata":{"id":"TqYKYs5Fir_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vit_object_detector.summary()"],"metadata":{"id":"YtaHEKaXi0zP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import plot_model\n","plot_model(vit_object_detector, to_file='model.png')"],"metadata":{"id":"h4d4GVXJi1D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train model\n","history = run_experiment(\n","                        vit_object_detector, LR, WEIGHT_DECAY, BATCH_SIZE, EPOCHS\n","                        )"],"metadata":{"id":"yll95giJi6v9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XKSZm_H2i62Y"},"execution_count":null,"outputs":[]}]}